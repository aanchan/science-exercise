{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checked, open or not-checkbox\n",
    "\n",
    "## Dataset description\n",
    "\n",
    "The dataset was provided as a series of PNG files in two folders. To get an understanding of the data a few shell commands were run and the information was dumped into a log file available in the Github repo as `dataset_info.txt`. The bash command was like so:\n",
    "\n",
    "`find . -iname \"*.png\" -type f | xargs -I '{}' file {} > dataset_info.txt`\n",
    "\n",
    "An example line from that file appears like so:\n",
    "\n",
    "`dataset-mleng-exercise/editor-1/{00D64925-EBFA-74BA-CD5A-2AB0D4A28E09}.pdf.results/checkbox-05.open.png: PNG image data, 15 x 13, 8-bit/color RGBA, non-interlaced`\n",
    "\n",
    "There are a total of 714 images. There were many PNG files starting with an `_` which seemed like artefacts from processing on a Macintosh computer which were removed.\n",
    "\n",
    "Of the 714 files, the class partitions for the three classes looked something like this:\n",
    "\n",
    "- checked: 187\n",
    "- not-checkbox: 214\n",
    "- open: 313\n",
    "\n",
    "The images are gray-scale though in three RGB channels. The files are of varying resolution. The minimum and maximum dimensions are as follows:\n",
    "\n",
    "1. Image height:\n",
    "    - Minimum : 8\n",
    "    - Maximum : 215\n",
    "2. Image Width : \n",
    "    - Minumum : 9\n",
    "    - Maximum : 130\n",
    "\n",
    "A visual inspection of the dataset was carried out using a system image viewer to understand the nature of the images.\n",
    "\n",
    "In order to feed the images into Keras the files had to be re-organized. Specifically the files from each class had to be moved into a folder of its own. For the three classes, three folders were created - checked, not-checkbox and open.\n",
    "\n",
    "The following bash code was used to put files into their respective folders.\n",
    "\n",
    "```\n",
    "class_name=not-checkbox #open, checked \n",
    "    for file in $(cat dataset-mleng-exercise/dataset_info.txt | cut -d: -f1 |   grep $class_name)\n",
    "      \n",
    "        do file_string=`echo $file | cut -d \"{\" -f2 | cut -d \"}\" -f1` fname=`echo $file | rev | cut -d/ -f1 | rev`\n",
    "        \n",
    "        cp $file dataset-mleng-exercise/$class_name/$file_string.$fname\n",
    "    \n",
    "    done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages common to multiple model implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, GlobalAveragePooling2D\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import tensorflow as tf\n",
    "%matplotlib notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set global variables\n",
    "\n",
    "### Note on input image resolution\n",
    "Here one can set the image height, image width and the batch size. Note that an image height and image width of 32 pixels was chosen as a compromise considering the variation in image sizes in the data set. This can and is a crucial hyperparameter to set as it controls the shape of the input tensor to the networks. If these dimensions are too small then detail in high resolution images is lost. On the other hand, if these dimensions are too large, then detail on small images gets fuzzy. Furthermore, the batch size is an important hyperparameter that has an effect on the gradient step size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height=32\n",
    "img_width=32\n",
    "batch_size = 32\n",
    "num_classes=3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models to run\n",
    "\n",
    "### Personal note on the exercise\n",
    "My PhD thesis was on transfer learning, and I discovered how easy things have gotten since I had to write generators and mark trainable layers on my own. A large part of this exercise was for me learning to use the new and shiny Keras API which has changed a lot since I last touched it. I first started this exercise with an example of MobileNet with this blog post as a reference (https://towardsdatascience.com/keras-transfer-learning-for-beginners-6c9b8b7143e). It turns out that MobileNet (Or InceptionV3 for example) is possibly not the best place to start since there are issues with transfer learning and normalization using BatchNorm which need some additional work. Here is a StackExchange post for reference: https://datascience.stackexchange.com/questions/47966/over-fitting-in-transfer-learning-with-small-dataset. BatchNorm it turns out learns statistics of the original data set. As a result the model really does not perform that well on the first run. This first exercise really is my quick and dirty way of exploring first in terms of what works and what does not work from a modelling aspect.\n",
    "\n",
    "### Notes on model selection\n",
    "Though this is not my favourite way of doing things, for the purposes of this exercise I explored three models to work with namely : \n",
    "- a vanilla convolutional network with very few parameters. I call this `model_1` in the variables below.\n",
    "- MobileNet as mentioned above. I call this `mobile_net` in the variables below.\n",
    "- VGG16 : Of interest in picking VGG16 were these two blog posts (https://riptutorial.com/keras/example/32608/transfer-learning-using-keras-and-vgg) and (https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html). I call this bool variable `vgg_net` below.\n",
    "\n",
    "## Step 1: Preprocessing functions\n",
    "I introduce these three variables below to pick the appropriate preprocessing functions before picking the data generators. For the vanilla convnet I settled at using a zoom_range of 0.2 and a horizonal flip since these seemed to make the largest difference on the validation error for this model. Given the time allotted to the exercise I did not dig deep into the implementation functions for MobileNet and VGGNet16.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 573 images belonging to 3 classes.\n",
      "Found 141 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "#Note: Only one of these can be set to True\n",
    "model_1=False\n",
    "mobile_net=False\n",
    "vgg_net=True\n",
    "\n",
    "dir_name = 'dataset'\n",
    "\n",
    "if model_1:\n",
    "    data_gen = ImageDataGenerator(rescale=1. / 255,\n",
    "                              zoom_range=0.2,\n",
    "                              horizontal_flip=True,\n",
    "                              validation_split=0.2)\n",
    "elif mobile_net:\n",
    "    from keras.applications.mobilenet import preprocess_input\n",
    "    data_gen = ImageDataGenerator(rescale=1. / 255,\n",
    "                                  preprocessing_function=preprocess_input,\n",
    "                                  validation_split=0.2)\n",
    "elif vgg_net:\n",
    "    from keras.applications.vgg16 import preprocess_input\n",
    "    data_gen = ImageDataGenerator(rescale=1. / 255,\n",
    "                                  preprocessing_function=preprocess_input,\n",
    "                                  validation_split=0.2)\n",
    "\n",
    "\n",
    "\n",
    "train_data_gen = data_gen.flow_from_directory(dir_name, subset='training',\n",
    "                                              target_size=(img_height,img_width),\n",
    "                                              batch_size=batch_size,\n",
    "                                              color_mode='rgb',\n",
    "                                              class_mode='categorical',\n",
    "                                              shuffle=True)\n",
    "\n",
    "validation_data_gen = data_gen.flow_from_directory(dir_name, subset='validation',\n",
    "                                                   target_size=(img_height,img_width),\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   color_mode='rgb',\n",
    "                                                   class_mode='categorical')\n",
    "step_size_train=train_data_gen.n//train_data_gen.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generators and validation data split\n",
    "Given the small amount of data I had initial thoughts of implementing k-fold cross validation. This it turns out is involved using a combination of keras and scikit-learn. K-fold cross-validation would be the ideal way to evaluate the performance of these models on this small data set. On the issues of data-sets, I thought this task comes under the broad-category of tasks named Document Image Classification. The exposition in this notebook is not ideal since I did not create an independent test set.\n",
    "\n",
    "## Additional datasets and data augmentation\n",
    "With reference to this blog post (https://medium.com/@dipti.rohan.pawar/document-classification-using-deep-learning-f9f5e31d4488) I found a dataset called Tobacco3482_dataset. This it turns out the link is dead to download the database. I did not explore obtaining additional datasets for this exercise in the interest of time, and settled towards using a train-validation split on the generators from Keras. Furthermore, the data can be certainly augmented by introducing \"noisy\" versions of the training images in the data set. This again was not pursued in the interest of time.\n",
    "\n",
    "## Relevant literature for future reading.\n",
    "An interesting direction to pursue is the work by Das et al. (https://github.com/hiarindam/document-image-classification-TL-SG) on intra-domain transfer learning using VGGNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 573 images belonging to 3 classes.\n",
      "Found 141 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize images in the training data set at the set resolution of img_height x img_width\n",
    "\n",
    "This is a stand-alone piece of code that allows visualizing all of the images in the training data set using the training data generator set above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "/* global mpl */\n",
       "window.mpl = {};\n",
       "\n",
       "mpl.get_websocket_type = function () {\n",
       "    if (typeof WebSocket !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof MozWebSocket !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert(\n",
       "            'Your browser does not have WebSocket support. ' +\n",
       "                'Please try Chrome, Safari or Firefox â‰¥ 6. ' +\n",
       "                'Firefox 4 and 5 are also supported but you ' +\n",
       "                'have to enable WebSockets in about:config.'\n",
       "        );\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure = function (figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = this.ws.binaryType !== undefined;\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById('mpl-warnings');\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent =\n",
       "                'This browser does not support binary websocket messages. ' +\n",
       "                'Performance may be slow.';\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = document.createElement('div');\n",
       "    this.root.setAttribute('style', 'display: inline-block');\n",
       "    this._root_extra_style(this.root);\n",
       "\n",
       "    parent_element.appendChild(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen = function () {\n",
       "        fig.send_message('supports_binary', { value: fig.supports_binary });\n",
       "        fig.send_message('send_image_mode', {});\n",
       "        if (fig.ratio !== 1) {\n",
       "            fig.send_message('set_dpi_ratio', { dpi_ratio: fig.ratio });\n",
       "        }\n",
       "        fig.send_message('refresh', {});\n",
       "    };\n",
       "\n",
       "    this.imageObj.onload = function () {\n",
       "        if (fig.image_mode === 'full') {\n",
       "            // Full images could contain transparency (where diff images\n",
       "            // almost always do), so we need to clear the canvas so that\n",
       "            // there is no ghosting.\n",
       "            fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "        }\n",
       "        fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "    };\n",
       "\n",
       "    this.imageObj.onunload = function () {\n",
       "        fig.ws.close();\n",
       "    };\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_header = function () {\n",
       "    var titlebar = document.createElement('div');\n",
       "    titlebar.classList =\n",
       "        'ui-dialog-titlebar ui-widget-header ui-corner-all ui-helper-clearfix';\n",
       "    var titletext = document.createElement('div');\n",
       "    titletext.classList = 'ui-dialog-title';\n",
       "    titletext.setAttribute(\n",
       "        'style',\n",
       "        'width: 100%; text-align: center; padding: 3px;'\n",
       "    );\n",
       "    titlebar.appendChild(titletext);\n",
       "    this.root.appendChild(titlebar);\n",
       "    this.header = titletext;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function (_canvas_div) {};\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function (_canvas_div) {};\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = (this.canvas_div = document.createElement('div'));\n",
       "    canvas_div.setAttribute(\n",
       "        'style',\n",
       "        'border: 1px solid #ddd;' +\n",
       "            'box-sizing: content-box;' +\n",
       "            'clear: both;' +\n",
       "            'min-height: 1px;' +\n",
       "            'min-width: 1px;' +\n",
       "            'outline: 0;' +\n",
       "            'overflow: hidden;' +\n",
       "            'position: relative;' +\n",
       "            'resize: both;'\n",
       "    );\n",
       "\n",
       "    function on_keyboard_event_closure(name) {\n",
       "        return function (event) {\n",
       "            return fig.key_event(event, name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    canvas_div.addEventListener(\n",
       "        'keydown',\n",
       "        on_keyboard_event_closure('key_press')\n",
       "    );\n",
       "    canvas_div.addEventListener(\n",
       "        'keyup',\n",
       "        on_keyboard_event_closure('key_release')\n",
       "    );\n",
       "\n",
       "    this._canvas_extra_style(canvas_div);\n",
       "    this.root.appendChild(canvas_div);\n",
       "\n",
       "    var canvas = (this.canvas = document.createElement('canvas'));\n",
       "    canvas.classList.add('mpl-canvas');\n",
       "    canvas.setAttribute('style', 'box-sizing: content-box;');\n",
       "\n",
       "    this.context = canvas.getContext('2d');\n",
       "\n",
       "    var backingStore =\n",
       "        this.context.backingStorePixelRatio ||\n",
       "        this.context.webkitBackingStorePixelRatio ||\n",
       "        this.context.mozBackingStorePixelRatio ||\n",
       "        this.context.msBackingStorePixelRatio ||\n",
       "        this.context.oBackingStorePixelRatio ||\n",
       "        this.context.backingStorePixelRatio ||\n",
       "        1;\n",
       "\n",
       "    this.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "    if (this.ratio !== 1) {\n",
       "        fig.send_message('set_dpi_ratio', { dpi_ratio: this.ratio });\n",
       "    }\n",
       "\n",
       "    var rubberband_canvas = (this.rubberband_canvas = document.createElement(\n",
       "        'canvas'\n",
       "    ));\n",
       "    rubberband_canvas.setAttribute(\n",
       "        'style',\n",
       "        'box-sizing: content-box; position: absolute; left: 0; top: 0; z-index: 1;'\n",
       "    );\n",
       "\n",
       "    var resizeObserver = new ResizeObserver(function (entries) {\n",
       "        var nentries = entries.length;\n",
       "        for (var i = 0; i < nentries; i++) {\n",
       "            var entry = entries[i];\n",
       "            var width, height;\n",
       "            if (entry.contentBoxSize) {\n",
       "                if (entry.contentBoxSize instanceof Array) {\n",
       "                    // Chrome 84 implements new version of spec.\n",
       "                    width = entry.contentBoxSize[0].inlineSize;\n",
       "                    height = entry.contentBoxSize[0].blockSize;\n",
       "                } else {\n",
       "                    // Firefox implements old version of spec.\n",
       "                    width = entry.contentBoxSize.inlineSize;\n",
       "                    height = entry.contentBoxSize.blockSize;\n",
       "                }\n",
       "            } else {\n",
       "                // Chrome <84 implements even older version of spec.\n",
       "                width = entry.contentRect.width;\n",
       "                height = entry.contentRect.height;\n",
       "            }\n",
       "\n",
       "            // Keep the size of the canvas and rubber band canvas in sync with\n",
       "            // the canvas container.\n",
       "            if (entry.devicePixelContentBoxSize) {\n",
       "                // Chrome 84 implements new version of spec.\n",
       "                canvas.setAttribute(\n",
       "                    'width',\n",
       "                    entry.devicePixelContentBoxSize[0].inlineSize\n",
       "                );\n",
       "                canvas.setAttribute(\n",
       "                    'height',\n",
       "                    entry.devicePixelContentBoxSize[0].blockSize\n",
       "                );\n",
       "            } else {\n",
       "                canvas.setAttribute('width', width * fig.ratio);\n",
       "                canvas.setAttribute('height', height * fig.ratio);\n",
       "            }\n",
       "            canvas.setAttribute(\n",
       "                'style',\n",
       "                'width: ' + width + 'px; height: ' + height + 'px;'\n",
       "            );\n",
       "\n",
       "            rubberband_canvas.setAttribute('width', width);\n",
       "            rubberband_canvas.setAttribute('height', height);\n",
       "\n",
       "            // And update the size in Python. We ignore the initial 0/0 size\n",
       "            // that occurs as the element is placed into the DOM, which should\n",
       "            // otherwise not happen due to the minimum size styling.\n",
       "            if (width != 0 && height != 0) {\n",
       "                fig.request_resize(width, height);\n",
       "            }\n",
       "        }\n",
       "    });\n",
       "    resizeObserver.observe(canvas_div);\n",
       "\n",
       "    function on_mouse_event_closure(name) {\n",
       "        return function (event) {\n",
       "            return fig.mouse_event(event, name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mousedown',\n",
       "        on_mouse_event_closure('button_press')\n",
       "    );\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseup',\n",
       "        on_mouse_event_closure('button_release')\n",
       "    );\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mousemove',\n",
       "        on_mouse_event_closure('motion_notify')\n",
       "    );\n",
       "\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseenter',\n",
       "        on_mouse_event_closure('figure_enter')\n",
       "    );\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseleave',\n",
       "        on_mouse_event_closure('figure_leave')\n",
       "    );\n",
       "\n",
       "    canvas_div.addEventListener('wheel', function (event) {\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        on_mouse_event_closure('scroll')(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.appendChild(canvas);\n",
       "    canvas_div.appendChild(rubberband_canvas);\n",
       "\n",
       "    this.rubberband_context = rubberband_canvas.getContext('2d');\n",
       "    this.rubberband_context.strokeStyle = '#000000';\n",
       "\n",
       "    this._resize_canvas = function (width, height, forward) {\n",
       "        if (forward) {\n",
       "            canvas_div.style.width = width + 'px';\n",
       "            canvas_div.style.height = height + 'px';\n",
       "        }\n",
       "    };\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    this.rubberband_canvas.addEventListener('contextmenu', function (_e) {\n",
       "        event.preventDefault();\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus() {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var toolbar = document.createElement('div');\n",
       "    toolbar.classList = 'mpl-toolbar';\n",
       "    this.root.appendChild(toolbar);\n",
       "\n",
       "    function on_click_closure(name) {\n",
       "        return function (_event) {\n",
       "            return fig.toolbar_button_onclick(name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    function on_mouseover_closure(tooltip) {\n",
       "        return function (event) {\n",
       "            if (!event.currentTarget.disabled) {\n",
       "                return fig.toolbar_button_onmouseover(tooltip);\n",
       "            }\n",
       "        };\n",
       "    }\n",
       "\n",
       "    fig.buttons = {};\n",
       "    var buttonGroup = document.createElement('div');\n",
       "    buttonGroup.classList = 'mpl-button-group';\n",
       "    for (var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            /* Instead of a spacer, we start a new button group. */\n",
       "            if (buttonGroup.hasChildNodes()) {\n",
       "                toolbar.appendChild(buttonGroup);\n",
       "            }\n",
       "            buttonGroup = document.createElement('div');\n",
       "            buttonGroup.classList = 'mpl-button-group';\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        var button = (fig.buttons[name] = document.createElement('button'));\n",
       "        button.classList = 'mpl-widget';\n",
       "        button.setAttribute('role', 'button');\n",
       "        button.setAttribute('aria-disabled', 'false');\n",
       "        button.addEventListener('click', on_click_closure(method_name));\n",
       "        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n",
       "\n",
       "        var icon_img = document.createElement('img');\n",
       "        icon_img.src = '_images/' + image + '.png';\n",
       "        icon_img.srcset = '_images/' + image + '_large.png 2x';\n",
       "        icon_img.alt = tooltip;\n",
       "        button.appendChild(icon_img);\n",
       "\n",
       "        buttonGroup.appendChild(button);\n",
       "    }\n",
       "\n",
       "    if (buttonGroup.hasChildNodes()) {\n",
       "        toolbar.appendChild(buttonGroup);\n",
       "    }\n",
       "\n",
       "    var fmt_picker = document.createElement('select');\n",
       "    fmt_picker.classList = 'mpl-widget';\n",
       "    toolbar.appendChild(fmt_picker);\n",
       "    this.format_dropdown = fmt_picker;\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = document.createElement('option');\n",
       "        option.selected = fmt === mpl.default_extension;\n",
       "        option.innerHTML = fmt;\n",
       "        fmt_picker.appendChild(option);\n",
       "    }\n",
       "\n",
       "    var status_bar = document.createElement('span');\n",
       "    status_bar.classList = 'mpl-message';\n",
       "    toolbar.appendChild(status_bar);\n",
       "    this.message = status_bar;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.request_resize = function (x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', { width: x_pixels, height: y_pixels });\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.send_message = function (type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function () {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({ type: 'draw', figure_id: this.id }));\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_save = function (fig, _msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function (fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] !== fig.canvas.width || size[1] !== fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1], msg['forward']);\n",
       "        fig.send_message('refresh', {});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function (fig, msg) {\n",
       "    var x0 = msg['x0'] / fig.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / fig.ratio;\n",
       "    var x1 = msg['x1'] / fig.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / fig.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0,\n",
       "        0,\n",
       "        fig.canvas.width / fig.ratio,\n",
       "        fig.canvas.height / fig.ratio\n",
       "    );\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function (fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function (fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch (cursor) {\n",
       "        case 0:\n",
       "            cursor = 'pointer';\n",
       "            break;\n",
       "        case 1:\n",
       "            cursor = 'default';\n",
       "            break;\n",
       "        case 2:\n",
       "            cursor = 'crosshair';\n",
       "            break;\n",
       "        case 3:\n",
       "            cursor = 'move';\n",
       "            break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_message = function (fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function (fig, _msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function (fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_history_buttons = function (fig, msg) {\n",
       "    for (var key in msg) {\n",
       "        if (!(key in fig.buttons)) {\n",
       "            continue;\n",
       "        }\n",
       "        fig.buttons[key].disabled = !msg[key];\n",
       "        fig.buttons[key].setAttribute('aria-disabled', !msg[key]);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_navigate_mode = function (fig, msg) {\n",
       "    if (msg['mode'] === 'PAN') {\n",
       "        fig.buttons['Pan'].classList.add('active');\n",
       "        fig.buttons['Zoom'].classList.remove('active');\n",
       "    } else if (msg['mode'] === 'ZOOM') {\n",
       "        fig.buttons['Pan'].classList.remove('active');\n",
       "        fig.buttons['Zoom'].classList.add('active');\n",
       "    } else {\n",
       "        fig.buttons['Pan'].classList.remove('active');\n",
       "        fig.buttons['Zoom'].classList.remove('active');\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function () {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message('ack', {});\n",
       "};\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function (fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = 'image/png';\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src\n",
       "                );\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data\n",
       "            );\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        } else if (\n",
       "            typeof evt.data === 'string' &&\n",
       "            evt.data.slice(0, 21) === 'data:image/png;base64'\n",
       "        ) {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig['handle_' + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\n",
       "                \"No handler for the '\" + msg_type + \"' message type: \",\n",
       "                msg\n",
       "            );\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\n",
       "                    \"Exception inside the 'handler_\" + msg_type + \"' callback:\",\n",
       "                    e,\n",
       "                    e.stack,\n",
       "                    msg\n",
       "                );\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "};\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function (e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e) {\n",
       "        e = window.event;\n",
       "    }\n",
       "    if (e.target) {\n",
       "        targ = e.target;\n",
       "    } else if (e.srcElement) {\n",
       "        targ = e.srcElement;\n",
       "    }\n",
       "    if (targ.nodeType === 3) {\n",
       "        // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "    }\n",
       "\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    var boundingRect = targ.getBoundingClientRect();\n",
       "    var x = e.pageX - (boundingRect.left + document.body.scrollLeft);\n",
       "    var y = e.pageY - (boundingRect.top + document.body.scrollTop);\n",
       "\n",
       "    return { x: x, y: y };\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys(original) {\n",
       "    return Object.keys(original).reduce(function (obj, key) {\n",
       "        if (typeof original[key] !== 'object') {\n",
       "            obj[key] = original[key];\n",
       "        }\n",
       "        return obj;\n",
       "    }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function (event, name) {\n",
       "    var canvas_pos = mpl.findpos(event);\n",
       "\n",
       "    if (name === 'button_press') {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * this.ratio;\n",
       "    var y = canvas_pos.y * this.ratio;\n",
       "\n",
       "    this.send_message(name, {\n",
       "        x: x,\n",
       "        y: y,\n",
       "        button: event.button,\n",
       "        step: event.step,\n",
       "        guiEvent: simpleKeys(event),\n",
       "    });\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function (_event, _name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.key_event = function (event, name) {\n",
       "    // Prevent repeat events\n",
       "    if (name === 'key_press') {\n",
       "        if (event.which === this._key) {\n",
       "            return;\n",
       "        } else {\n",
       "            this._key = event.which;\n",
       "        }\n",
       "    }\n",
       "    if (name === 'key_release') {\n",
       "        this._key = null;\n",
       "    }\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which !== 17) {\n",
       "        value += 'ctrl+';\n",
       "    }\n",
       "    if (event.altKey && event.which !== 18) {\n",
       "        value += 'alt+';\n",
       "    }\n",
       "    if (event.shiftKey && event.which !== 16) {\n",
       "        value += 'shift+';\n",
       "    }\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, { key: value, guiEvent: simpleKeys(event) });\n",
       "    return false;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function (name) {\n",
       "    if (name === 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message('toolbar_button', { name: name });\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function (tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Left button pans, Right button zooms\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";/* global mpl */\n",
       "\n",
       "var comm_websocket_adapter = function (comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function () {\n",
       "        comm.close();\n",
       "    };\n",
       "    ws.send = function (m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function (msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data']);\n",
       "    });\n",
       "    return ws;\n",
       "};\n",
       "\n",
       "mpl.mpl_figure_comm = function (comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = document.getElementById(id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm);\n",
       "\n",
       "    function ondownload(figure, _format) {\n",
       "        window.open(figure.canvas.toDataURL());\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy, ondownload, element);\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element;\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error('Failed to find cell for figure', id, fig);\n",
       "        return;\n",
       "    }\n",
       "    fig.cell_info[0].output_area.element.one(\n",
       "        'cleared',\n",
       "        { fig: fig },\n",
       "        fig._remove_fig_handler\n",
       "    );\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function (fig, msg) {\n",
       "    var width = fig.canvas.width / fig.ratio;\n",
       "    fig.cell_info[0].output_area.element.off(\n",
       "        'cleared',\n",
       "        fig._remove_fig_handler\n",
       "    );\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable();\n",
       "    fig.parent_element.innerHTML =\n",
       "        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "    fig.close_ws(fig, msg);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.close_ws = function (fig, msg) {\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function (_remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width / this.ratio;\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] =\n",
       "        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function () {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message('ack', {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () {\n",
       "        fig.push_to_output();\n",
       "    }, 1000);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var toolbar = document.createElement('div');\n",
       "    toolbar.classList = 'btn-toolbar';\n",
       "    this.root.appendChild(toolbar);\n",
       "\n",
       "    function on_click_closure(name) {\n",
       "        return function (_event) {\n",
       "            return fig.toolbar_button_onclick(name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    function on_mouseover_closure(tooltip) {\n",
       "        return function (event) {\n",
       "            if (!event.currentTarget.disabled) {\n",
       "                return fig.toolbar_button_onmouseover(tooltip);\n",
       "            }\n",
       "        };\n",
       "    }\n",
       "\n",
       "    fig.buttons = {};\n",
       "    var buttonGroup = document.createElement('div');\n",
       "    buttonGroup.classList = 'btn-group';\n",
       "    var button;\n",
       "    for (var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            /* Instead of a spacer, we start a new button group. */\n",
       "            if (buttonGroup.hasChildNodes()) {\n",
       "                toolbar.appendChild(buttonGroup);\n",
       "            }\n",
       "            buttonGroup = document.createElement('div');\n",
       "            buttonGroup.classList = 'btn-group';\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        button = fig.buttons[name] = document.createElement('button');\n",
       "        button.classList = 'btn btn-default';\n",
       "        button.href = '#';\n",
       "        button.title = name;\n",
       "        button.innerHTML = '<i class=\"fa ' + image + ' fa-lg\"></i>';\n",
       "        button.addEventListener('click', on_click_closure(method_name));\n",
       "        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n",
       "        buttonGroup.appendChild(button);\n",
       "    }\n",
       "\n",
       "    if (buttonGroup.hasChildNodes()) {\n",
       "        toolbar.appendChild(buttonGroup);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = document.createElement('span');\n",
       "    status_bar.classList = 'mpl-message pull-right';\n",
       "    toolbar.appendChild(status_bar);\n",
       "    this.message = status_bar;\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = document.createElement('div');\n",
       "    buttongrp.classList = 'btn-group inline pull-right';\n",
       "    button = document.createElement('button');\n",
       "    button.classList = 'btn btn-mini btn-primary';\n",
       "    button.href = '#';\n",
       "    button.title = 'Stop Interaction';\n",
       "    button.innerHTML = '<i class=\"fa fa-power-off icon-remove icon-large\"></i>';\n",
       "    button.addEventListener('click', function (_evt) {\n",
       "        fig.handle_close(fig, {});\n",
       "    });\n",
       "    button.addEventListener(\n",
       "        'mouseover',\n",
       "        on_mouseover_closure('Stop Interaction')\n",
       "    );\n",
       "    buttongrp.appendChild(button);\n",
       "    var titlebar = this.root.querySelector('.ui-dialog-titlebar');\n",
       "    titlebar.insertBefore(buttongrp, titlebar.firstChild);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._remove_fig_handler = function (event) {\n",
       "    var fig = event.data.fig;\n",
       "    fig.close_ws(fig, {});\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function (el) {\n",
       "    el.style.boxSizing = 'content-box'; // override notebook setting of border-box.\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function (el) {\n",
       "    // this is important to make the div 'focusable\n",
       "    el.setAttribute('tabindex', 0);\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    } else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function (event, _name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager) {\n",
       "        manager = IPython.keyboard_manager;\n",
       "    }\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which === 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_save = function (fig, _msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "};\n",
       "\n",
       "mpl.find_output_cell = function (html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i = 0; i < ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code') {\n",
       "            for (var j = 0; j < cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] === html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "};\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel !== null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target(\n",
       "        'matplotlib',\n",
       "        mpl.mpl_figure_comm\n",
       "    );\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"639.7499809339648\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 1 to continue, 0 to break : 0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "class_dict = {'0':'checked', '1': 'not-checkbox', '2': 'open'}\n",
    "outer_loop_break_flag = False\n",
    "for x_batch, y_batch in train_data_gen:\n",
    "    if not outer_loop_break_flag:\n",
    "        for i in range (0,batch_size):\n",
    "            plt.ion()\n",
    "            image = np.clip(x_batch[i],0,1)\n",
    "            plt.title('Class:' + class_dict[str(np.argmax(y_batch[i]))])\n",
    "            ax.imshow(image)\n",
    "            plt.gcf().canvas.draw()\n",
    "            yorn = input(\"Press 1 to continue, 0 to break : \") \n",
    "            print(yorn)\n",
    "            if yorn == '0':\n",
    "                outer_loop_break_flag = True\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 a: Model 1: Vanilla Convnet - up to 70-75% accuracy on a limited validation set.\n",
    "\n",
    "In order to run this step make sure that `model_1 = True` in Step 1 above.\n",
    "\n",
    "This step runs the vanilla convnet which has two convolutional layers with max-pooling followed by a dense layer with softmax. Given the small amount of data a simple classifier often is a good first place to start in order to be able to see what classes get confused the most and so on. The model was tweaked a little bit by looking at the performance of the model on the validation set. For example a batch size of 16 seemed to make a performance hit, vs trying kernel sizes of (3,3) vs (5,5). A kernel size of (5,5) was finally chosen for performance reasons. Dropout seemed to make a difference in the final dense layer and a value of 0.5 seemed to be better than 0.25. Furthermore, a callback function was written to save the weights locally depending on the model that gave the best validation performance over 50 epochs. In addition I initially tried two dense layers vs the one that is currently in the model. The validation performance was fairly poor since the model was overparameterized and was overfitting. I tried changing 'Adam' vs 'RMSProp' as well - Adam anecdotally seemed to perform better. One can see with this simple model and the validation accuracy numbers that one can reach perhaps an accuracy of 70-75% on this limited validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-67f207e92de1>:25: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 1s 53ms/step - loss: 1.0968 - accuracy: 0.4011 - val_loss: 1.0738 - val_accuracy: 0.4397\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 1s 40ms/step - loss: 1.0725 - accuracy: 0.4399 - val_loss: 1.0544 - val_accuracy: 0.4468\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 1s 38ms/step - loss: 1.0694 - accuracy: 0.4603 - val_loss: 1.0255 - val_accuracy: 0.5248\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 1s 36ms/step - loss: 1.0193 - accuracy: 0.4917 - val_loss: 0.9433 - val_accuracy: 0.6028\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 1s 38ms/step - loss: 0.9747 - accuracy: 0.5287 - val_loss: 0.9166 - val_accuracy: 0.5674\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 1s 37ms/step - loss: 0.9159 - accuracy: 0.5619 - val_loss: 0.9046 - val_accuracy: 0.5674\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 1s 38ms/step - loss: 0.9342 - accuracy: 0.5712 - val_loss: 0.9252 - val_accuracy: 0.5177\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 1s 39ms/step - loss: 0.9183 - accuracy: 0.5970 - val_loss: 0.8147 - val_accuracy: 0.6028\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 1s 35ms/step - loss: 0.8620 - accuracy: 0.6118 - val_loss: 0.7749 - val_accuracy: 0.6170\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 1s 39ms/step - loss: 0.8078 - accuracy: 0.6359 - val_loss: 0.8016 - val_accuracy: 0.6170\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 1s 37ms/step - loss: 0.8402 - accuracy: 0.6340 - val_loss: 0.8099 - val_accuracy: 0.6454\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 1s 37ms/step - loss: 0.7827 - accuracy: 0.6377 - val_loss: 0.8754 - val_accuracy: 0.5745\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 1s 38ms/step - loss: 0.7352 - accuracy: 0.6784 - val_loss: 0.7538 - val_accuracy: 0.6383\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 1s 38ms/step - loss: 0.7394 - accuracy: 0.6821 - val_loss: 0.7729 - val_accuracy: 0.6596\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 1s 36ms/step - loss: 0.7045 - accuracy: 0.7116 - val_loss: 0.7325 - val_accuracy: 0.6383\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 1s 37ms/step - loss: 0.6834 - accuracy: 0.7190 - val_loss: 0.7269 - val_accuracy: 0.6383\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 1s 37ms/step - loss: 0.6764 - accuracy: 0.7153 - val_loss: 0.7297 - val_accuracy: 0.6596\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 1s 36ms/step - loss: 0.6262 - accuracy: 0.7449 - val_loss: 0.6610 - val_accuracy: 0.6809\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 1s 37ms/step - loss: 0.6317 - accuracy: 0.7338 - val_loss: 0.6867 - val_accuracy: 0.6879\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 1s 39ms/step - loss: 0.6242 - accuracy: 0.7486 - val_loss: 0.6750 - val_accuracy: 0.6809\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 1s 38ms/step - loss: 0.5894 - accuracy: 0.7542 - val_loss: 0.6772 - val_accuracy: 0.6596\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 1s 38ms/step - loss: 0.5390 - accuracy: 0.7967 - val_loss: 0.6321 - val_accuracy: 0.6879\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 1s 39ms/step - loss: 0.5540 - accuracy: 0.7782 - val_loss: 0.6177 - val_accuracy: 0.7092\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 1s 39ms/step - loss: 0.5815 - accuracy: 0.7763 - val_loss: 0.6069 - val_accuracy: 0.6950\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 1s 38ms/step - loss: 0.5963 - accuracy: 0.7542 - val_loss: 0.7198 - val_accuracy: 0.6667\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 1s 39ms/step - loss: 0.5831 - accuracy: 0.7689 - val_loss: 0.6545 - val_accuracy: 0.6879\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 1s 41ms/step - loss: 0.5132 - accuracy: 0.7911 - val_loss: 0.6166 - val_accuracy: 0.6950\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 1s 40ms/step - loss: 0.5151 - accuracy: 0.8059 - val_loss: 0.6629 - val_accuracy: 0.7163\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 1s 39ms/step - loss: 0.5011 - accuracy: 0.8152 - val_loss: 0.6936 - val_accuracy: 0.6950\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 1s 39ms/step - loss: 0.4506 - accuracy: 0.8152 - val_loss: 0.7382 - val_accuracy: 0.6667\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 1s 41ms/step - loss: 0.4703 - accuracy: 0.8170 - val_loss: 1.0142 - val_accuracy: 0.5248\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 1s 40ms/step - loss: 0.5357 - accuracy: 0.7967 - val_loss: 0.6554 - val_accuracy: 0.7163\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 1s 40ms/step - loss: 0.5011 - accuracy: 0.8207 - val_loss: 0.7618 - val_accuracy: 0.6738\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 1s 40ms/step - loss: 0.5255 - accuracy: 0.8281 - val_loss: 0.9180 - val_accuracy: 0.5390\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 1s 40ms/step - loss: 0.5137 - accuracy: 0.8189 - val_loss: 0.6734 - val_accuracy: 0.7021\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 1s 43ms/step - loss: 0.5428 - accuracy: 0.8004 - val_loss: 0.6736 - val_accuracy: 0.7305\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 1s 39ms/step - loss: 0.4804 - accuracy: 0.8022 - val_loss: 0.5947 - val_accuracy: 0.7163\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 1s 46ms/step - loss: 0.4371 - accuracy: 0.8355 - val_loss: 0.6831 - val_accuracy: 0.7021\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 1s 45ms/step - loss: 0.4675 - accuracy: 0.8059 - val_loss: 0.6134 - val_accuracy: 0.6950\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 1s 44ms/step - loss: 0.4401 - accuracy: 0.8336 - val_loss: 0.6149 - val_accuracy: 0.7021\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 1s 46ms/step - loss: 0.4392 - accuracy: 0.8466 - val_loss: 0.6699 - val_accuracy: 0.6809\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 1s 42ms/step - loss: 0.4313 - accuracy: 0.8429 - val_loss: 0.6420 - val_accuracy: 0.7092\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 1s 41ms/step - loss: 0.4024 - accuracy: 0.8632 - val_loss: 0.7865 - val_accuracy: 0.6950\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 1s 43ms/step - loss: 0.4954 - accuracy: 0.8170 - val_loss: 0.7638 - val_accuracy: 0.6596\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 1s 45ms/step - loss: 0.4540 - accuracy: 0.8484 - val_loss: 0.6384 - val_accuracy: 0.7447\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 1s 48ms/step - loss: 0.4123 - accuracy: 0.8558 - val_loss: 0.6447 - val_accuracy: 0.6950\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 0.3934 - accuracy: 0.8688 - val_loss: 0.6696 - val_accuracy: 0.7518\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 1s 43ms/step - loss: 0.4013 - accuracy: 0.8706 - val_loss: 0.6775 - val_accuracy: 0.7021\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 1s 45ms/step - loss: 0.4091 - accuracy: 0.8614 - val_loss: 0.8608 - val_accuracy: 0.6525\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 1s 41ms/step - loss: 0.4432 - accuracy: 0.8364 - val_loss: 0.6672 - val_accuracy: 0.7305\n"
     ]
    }
   ],
   "source": [
    "if model_1:\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=16, kernel_size=(5, 5), activation=\"relu\", input_shape=(img_height,img_width,3)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters=32, kernel_size=(5, 5), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    checkpoint_filepath = 'weights_1'\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        save_weights_only=True,\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True)\n",
    "\n",
    "    model.fit_generator(train_data_gen,\n",
    "                        steps_per_epoch=step_size_train,\n",
    "                        epochs=50,\n",
    "                        validation_data=validation_data_gen,\n",
    "                        callbacks=model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance\n",
    "\n",
    "Understanding model performance is often an important part of the workflow. While validation scores and training and validation error curves are great to look at to see if training is progressing correctly, it is often informative to look at a confusion matrix and metrics other than accuracy in especially detection problems and look at things like precision, recall and f1-score. For this exercise I often found through the use of the confusion matrix that my model was actually never really learning anything. For a bad training run where my model really wasn't really learning anything I found examples from all classes being classified as a single class. From the confusion matrix below on the validation set one can see that the model is at least learning \"something\", but more so see where the model is lacking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-5e2de010cca9>:1: Model.predict_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.predict, which supports generators.\n",
      "Confusion Matrix\n",
      "[[13 11 13]\n",
      " [10 13 19]\n",
      " [16 18 28]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     checked       0.33      0.35      0.34        37\n",
      "not-checkbox       0.31      0.31      0.31        42\n",
      "        open       0.47      0.45      0.46        62\n",
      "\n",
      "    accuracy                           0.38       141\n",
      "   macro avg       0.37      0.37      0.37       141\n",
      "weighted avg       0.38      0.38      0.38       141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict_generator(validation_data_gen)\n",
    "y_pred = np.argmax(preds, axis=1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(validation_data_gen.classes, y_pred))\n",
    "print('Classification Report')\n",
    "target_names = ['checked', 'not-checkbox', 'open']\n",
    "print(classification_report(validation_data_gen.classes, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2b : MobileNet - 60-65% on a limited validation set\n",
    "\n",
    "In order to run this step make sure that `mobile_net = True` in Step 1 above.\n",
    "\n",
    "One of the things (among many) that I learned while playing around with these pre-trained models was that the input image tensor size was fixed to a particular size. For example MobileNet has its (image_width, image_height) set to (224,224). This can often be an issue with low-resolution images such as the one provided for this exercise. As a result the input tensor often needs to be changed. There are multiple blog posts which talk about this issue, such as https://www.pyimagesearch.com/2019/06/24/change-input-shape-dimensions-for-fine-tuning-with-keras/.  What appears below is a `change_model` function that was taken from [this](https://medium.com/@ckyrkou/changing-input-size-of-pre-trained-models-in-keras-3dfbe3ca3091) blog-post. As mentioned earlier, this model does not give the best validation set performance. One reason could be models of this class that use BatchNorm could be remebering statistics of the original dataset instead. Furthermore, checking the accuracy of the input tensor, and the weights that are finally loaded and convolved need to be checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Loaded layer input_4\n",
      "Loaded layer conv1_pad\n",
      "Loaded layer conv1\n",
      "Loaded layer conv1_bn\n",
      "Loaded layer conv1_relu\n",
      "Loaded layer conv_dw_1\n",
      "Loaded layer conv_dw_1_bn\n",
      "Loaded layer conv_dw_1_relu\n",
      "Loaded layer conv_pw_1\n",
      "Loaded layer conv_pw_1_bn\n",
      "Loaded layer conv_pw_1_relu\n",
      "Loaded layer conv_pad_2\n",
      "Loaded layer conv_dw_2\n",
      "Loaded layer conv_dw_2_bn\n",
      "Loaded layer conv_dw_2_relu\n",
      "Loaded layer conv_pw_2\n",
      "Loaded layer conv_pw_2_bn\n",
      "Loaded layer conv_pw_2_relu\n",
      "Loaded layer conv_dw_3\n",
      "Loaded layer conv_dw_3_bn\n",
      "Loaded layer conv_dw_3_relu\n",
      "Loaded layer conv_pw_3\n",
      "Loaded layer conv_pw_3_bn\n",
      "Loaded layer conv_pw_3_relu\n",
      "Loaded layer conv_pad_4\n",
      "Loaded layer conv_dw_4\n",
      "Loaded layer conv_dw_4_bn\n",
      "Loaded layer conv_dw_4_relu\n",
      "Loaded layer conv_pw_4\n",
      "Loaded layer conv_pw_4_bn\n",
      "Loaded layer conv_pw_4_relu\n",
      "Loaded layer conv_dw_5\n",
      "Loaded layer conv_dw_5_bn\n",
      "Loaded layer conv_dw_5_relu\n",
      "Loaded layer conv_pw_5\n",
      "Loaded layer conv_pw_5_bn\n",
      "Loaded layer conv_pw_5_relu\n",
      "Loaded layer conv_pad_6\n",
      "Loaded layer conv_dw_6\n",
      "Loaded layer conv_dw_6_bn\n",
      "Loaded layer conv_dw_6_relu\n",
      "Loaded layer conv_pw_6\n",
      "Loaded layer conv_pw_6_bn\n",
      "Loaded layer conv_pw_6_relu\n",
      "Loaded layer conv_dw_7\n",
      "Loaded layer conv_dw_7_bn\n",
      "Loaded layer conv_dw_7_relu\n",
      "Loaded layer conv_pw_7\n",
      "Loaded layer conv_pw_7_bn\n",
      "Loaded layer conv_pw_7_relu\n",
      "Loaded layer conv_dw_8\n",
      "Loaded layer conv_dw_8_bn\n",
      "Loaded layer conv_dw_8_relu\n",
      "Loaded layer conv_pw_8\n",
      "Loaded layer conv_pw_8_bn\n",
      "Loaded layer conv_pw_8_relu\n",
      "Loaded layer conv_dw_9\n",
      "Loaded layer conv_dw_9_bn\n",
      "Loaded layer conv_dw_9_relu\n",
      "Loaded layer conv_pw_9\n",
      "Loaded layer conv_pw_9_bn\n",
      "Loaded layer conv_pw_9_relu\n",
      "Loaded layer conv_dw_10\n",
      "Loaded layer conv_dw_10_bn\n",
      "Loaded layer conv_dw_10_relu\n",
      "Loaded layer conv_pw_10\n",
      "Loaded layer conv_pw_10_bn\n",
      "Loaded layer conv_pw_10_relu\n",
      "Loaded layer conv_dw_11\n",
      "Loaded layer conv_dw_11_bn\n",
      "Loaded layer conv_dw_11_relu\n",
      "Loaded layer conv_pw_11\n",
      "Loaded layer conv_pw_11_bn\n",
      "Loaded layer conv_pw_11_relu\n",
      "Loaded layer conv_pad_12\n",
      "Loaded layer conv_dw_12\n",
      "Loaded layer conv_dw_12_bn\n",
      "Loaded layer conv_dw_12_relu\n",
      "Loaded layer conv_pw_12\n",
      "Loaded layer conv_pw_12_bn\n",
      "Loaded layer conv_pw_12_relu\n",
      "Loaded layer conv_dw_13\n",
      "Loaded layer conv_dw_13_bn\n",
      "Loaded layer conv_dw_13_relu\n",
      "Loaded layer conv_pw_13\n",
      "Loaded layer conv_pw_13_bn\n",
      "Loaded layer conv_pw_13_relu\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 1.1811 - accuracy: 0.6858 - val_loss: 1.0739 - val_accuracy: 0.4397\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 2s 105ms/step - loss: 0.7576 - accuracy: 0.7985 - val_loss: 1.0661 - val_accuracy: 0.4255\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 2s 104ms/step - loss: 0.5157 - accuracy: 0.8355 - val_loss: 1.0455 - val_accuracy: 0.4752\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.3986 - accuracy: 0.8577 - val_loss: 1.2473 - val_accuracy: 0.4681\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.2941 - accuracy: 0.8872 - val_loss: 1.4959 - val_accuracy: 0.5106\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 2s 127ms/step - loss: 0.2133 - accuracy: 0.9205 - val_loss: 1.4220 - val_accuracy: 0.5106\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 2s 136ms/step - loss: 0.2172 - accuracy: 0.9261 - val_loss: 1.5725 - val_accuracy: 0.4965\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 2s 137ms/step - loss: 0.2509 - accuracy: 0.9076 - val_loss: 1.1508 - val_accuracy: 0.5745\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 2s 135ms/step - loss: 0.2158 - accuracy: 0.9224 - val_loss: 1.2942 - val_accuracy: 0.5035\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 0.1659 - accuracy: 0.9464 - val_loss: 1.7223 - val_accuracy: 0.4610\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 2s 135ms/step - loss: 0.1376 - accuracy: 0.9519 - val_loss: 1.3586 - val_accuracy: 0.5177\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.2175 - accuracy: 0.9242 - val_loss: 1.3974 - val_accuracy: 0.5532\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.1481 - accuracy: 0.9575 - val_loss: 1.2017 - val_accuracy: 0.5887\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 2s 141ms/step - loss: 0.0994 - accuracy: 0.9667 - val_loss: 0.8798 - val_accuracy: 0.6525\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 2s 130ms/step - loss: 0.1991 - accuracy: 0.9316 - val_loss: 1.3096 - val_accuracy: 0.5816\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 2s 143ms/step - loss: 0.1880 - accuracy: 0.9316 - val_loss: 1.2362 - val_accuracy: 0.6099\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 2s 130ms/step - loss: 0.1110 - accuracy: 0.9538 - val_loss: 1.2751 - val_accuracy: 0.6170\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.1640 - accuracy: 0.9538 - val_loss: 1.1671 - val_accuracy: 0.6170\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.1118 - accuracy: 0.9704 - val_loss: 1.4398 - val_accuracy: 0.5887\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.0940 - accuracy: 0.9704 - val_loss: 1.2306 - val_accuracy: 0.6028\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.1041 - accuracy: 0.9630 - val_loss: 1.4112 - val_accuracy: 0.6738\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 2s 132ms/step - loss: 0.1341 - accuracy: 0.9612 - val_loss: 1.2853 - val_accuracy: 0.6879\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 2s 131ms/step - loss: 0.0838 - accuracy: 0.9741 - val_loss: 1.5736 - val_accuracy: 0.6028\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 3s 155ms/step - loss: 0.1031 - accuracy: 0.9667 - val_loss: 1.5371 - val_accuracy: 0.5816\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 2s 132ms/step - loss: 0.1172 - accuracy: 0.9630 - val_loss: 1.5019 - val_accuracy: 0.5816\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 2s 136ms/step - loss: 0.0946 - accuracy: 0.9686 - val_loss: 1.5464 - val_accuracy: 0.5674\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 2s 143ms/step - loss: 0.0998 - accuracy: 0.9630 - val_loss: 2.0908 - val_accuracy: 0.5248\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 2s 130ms/step - loss: 0.0973 - accuracy: 0.9686 - val_loss: 1.4733 - val_accuracy: 0.5887\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 2s 134ms/step - loss: 0.1150 - accuracy: 0.9649 - val_loss: 1.8438 - val_accuracy: 0.6170\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 2s 133ms/step - loss: 0.1347 - accuracy: 0.9482 - val_loss: 1.7962 - val_accuracy: 0.5603\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 2s 129ms/step - loss: 0.1346 - accuracy: 0.9538 - val_loss: 2.3309 - val_accuracy: 0.5461\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 2s 129ms/step - loss: 0.0983 - accuracy: 0.9686 - val_loss: 1.8809 - val_accuracy: 0.5674\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 2s 131ms/step - loss: 0.0843 - accuracy: 0.9741 - val_loss: 1.8413 - val_accuracy: 0.5390\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 2s 131ms/step - loss: 0.1008 - accuracy: 0.9630 - val_loss: 2.6458 - val_accuracy: 0.5319\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 2s 133ms/step - loss: 0.0955 - accuracy: 0.9630 - val_loss: 1.9652 - val_accuracy: 0.6241\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 2s 132ms/step - loss: 0.0513 - accuracy: 0.9889 - val_loss: 1.9149 - val_accuracy: 0.6241\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 2s 134ms/step - loss: 0.0539 - accuracy: 0.9871 - val_loss: 1.9866 - val_accuracy: 0.5603\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 2s 141ms/step - loss: 0.0779 - accuracy: 0.9760 - val_loss: 2.0473 - val_accuracy: 0.6170\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 2s 140ms/step - loss: 0.0456 - accuracy: 0.9852 - val_loss: 1.8498 - val_accuracy: 0.5887\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 2s 130ms/step - loss: 0.0650 - accuracy: 0.9778 - val_loss: 1.6811 - val_accuracy: 0.6950\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 2s 121ms/step - loss: 0.0792 - accuracy: 0.9686 - val_loss: 2.3728 - val_accuracy: 0.5674\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.1412 - accuracy: 0.9538 - val_loss: 1.9519 - val_accuracy: 0.6312\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 0.1207 - accuracy: 0.9704 - val_loss: 1.9855 - val_accuracy: 0.6170\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.0970 - accuracy: 0.9667 - val_loss: 2.1371 - val_accuracy: 0.6028\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.0507 - accuracy: 0.9852 - val_loss: 3.2195 - val_accuracy: 0.5319\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.0498 - accuracy: 0.9741 - val_loss: 2.6511 - val_accuracy: 0.5390\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.0768 - accuracy: 0.9723 - val_loss: 2.0782 - val_accuracy: 0.5674\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.0785 - accuracy: 0.9778 - val_loss: 1.6929 - val_accuracy: 0.5887\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.0669 - accuracy: 0.9741 - val_loss: 1.6564 - val_accuracy: 0.6667\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.0745 - accuracy: 0.9724 - val_loss: 1.5104 - val_accuracy: 0.6879\n"
     ]
    }
   ],
   "source": [
    "def change_model(model, new_input_shape=(None, img_width, img_height, 3)):\n",
    "    # replace input shape of first layer\n",
    "    model._layers[0].batch_input_shape = new_input_shape\n",
    "\n",
    "    # rebuild model architecture by exporting and importing via json\n",
    "    new_model = keras.models.model_from_json(model.to_json())\n",
    "\n",
    "    # copy weights from old model to new one\n",
    "    for layer in new_model.layers:\n",
    "        try:\n",
    "            layer.set_weights(model.get_layer(name=layer.name).get_weights())\n",
    "            print(\"Loaded layer {}\".format(layer.name))\n",
    "        except:\n",
    "            print(\"Could not transfer weights for layer {}\".format(layer.name))\n",
    "\n",
    "    return new_model\n",
    "\n",
    "if mobile_net:\n",
    "    from keras.applications import MobileNet\n",
    "    from keras.layers import Dense\n",
    "    from keras.models import Model\n",
    "    base_model = MobileNet(weights='imagenet',\n",
    "                           include_top=False)\n",
    "\n",
    "    base_model_changed = change_model(base_model)\n",
    "    x=base_model_changed.output\n",
    "    x=GlobalAveragePooling2D()(x)\n",
    "    x=Dense(512,activation='relu')(x) #dense layer 3\n",
    "    preds=Dense(num_classes,activation='softmax')(x) #final layer with softmax activation\n",
    "    model=Model(base_model_changed.input, preds)\n",
    "    for layer in model.layers[:80]:\n",
    "        layer.trainable = False\n",
    "    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit_generator(train_data_gen,\n",
    "                        steps_per_epoch=step_size_train,\n",
    "                        epochs=50,\n",
    "                        validation_data=validation_data_gen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2c : VGGNet16 - 70-75% out of the box\n",
    "\n",
    "Make sure that in Step 1 `vgg_net = True`\n",
    "\n",
    "As mentioned in [this tutorial](https://riptutorial.com/keras/example/32608/transfer-learning-using-keras-and-vgg) loading the VGGNet model supports an argument for the input_tensor. Therefore one did not need to include a `change_model` function as a part of the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "17/17 [==============================] - 2s 89ms/step - loss: 12.0398 - accuracy: 0.3641 - val_loss: 1.3680 - val_accuracy: 0.5177\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 1s 72ms/step - loss: 1.2059 - accuracy: 0.5102 - val_loss: 0.9133 - val_accuracy: 0.5674\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 1s 75ms/step - loss: 0.8949 - accuracy: 0.5915 - val_loss: 0.8853 - val_accuracy: 0.6028\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 1s 78ms/step - loss: 0.6713 - accuracy: 0.7098 - val_loss: 0.8062 - val_accuracy: 0.6241\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 1s 78ms/step - loss: 0.5487 - accuracy: 0.7726 - val_loss: 0.9274 - val_accuracy: 0.6170\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 1s 80ms/step - loss: 0.5078 - accuracy: 0.8004 - val_loss: 0.7396 - val_accuracy: 0.6738\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 1s 84ms/step - loss: 0.4605 - accuracy: 0.8373 - val_loss: 0.7577 - val_accuracy: 0.7163\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 1s 84ms/step - loss: 0.4230 - accuracy: 0.8540 - val_loss: 0.8416 - val_accuracy: 0.6525\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 1s 86ms/step - loss: 0.2766 - accuracy: 0.8909 - val_loss: 1.1196 - val_accuracy: 0.6950\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 1s 87ms/step - loss: 0.2699 - accuracy: 0.9002 - val_loss: 0.8401 - val_accuracy: 0.6950\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 2s 91ms/step - loss: 0.1900 - accuracy: 0.9353 - val_loss: 1.1675 - val_accuracy: 0.6596\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 2s 90ms/step - loss: 0.1559 - accuracy: 0.9427 - val_loss: 1.1871 - val_accuracy: 0.6312\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 2s 90ms/step - loss: 0.1438 - accuracy: 0.9519 - val_loss: 1.0450 - val_accuracy: 0.6950\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 2s 93ms/step - loss: 0.1527 - accuracy: 0.9464 - val_loss: 1.2951 - val_accuracy: 0.6950\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 2s 92ms/step - loss: 0.0977 - accuracy: 0.9723 - val_loss: 1.3361 - val_accuracy: 0.6738\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 2s 94ms/step - loss: 0.1744 - accuracy: 0.9409 - val_loss: 1.1596 - val_accuracy: 0.6809\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 2s 95ms/step - loss: 0.1567 - accuracy: 0.9372 - val_loss: 1.2949 - val_accuracy: 0.6738\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 2s 93ms/step - loss: 0.1214 - accuracy: 0.9556 - val_loss: 1.4127 - val_accuracy: 0.7092\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 2s 103ms/step - loss: 0.0854 - accuracy: 0.9704 - val_loss: 1.2553 - val_accuracy: 0.7447\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.1159 - accuracy: 0.9575 - val_loss: 1.4587 - val_accuracy: 0.6667\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.1074 - accuracy: 0.9575 - val_loss: 1.3510 - val_accuracy: 0.6667\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.0724 - accuracy: 0.9760 - val_loss: 1.5699 - val_accuracy: 0.7092\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 2s 104ms/step - loss: 0.0528 - accuracy: 0.9778 - val_loss: 1.3275 - val_accuracy: 0.7092\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 2s 104ms/step - loss: 0.0564 - accuracy: 0.9797 - val_loss: 1.8366 - val_accuracy: 0.6667\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 2s 103ms/step - loss: 0.0339 - accuracy: 0.9908 - val_loss: 1.4830 - val_accuracy: 0.7163\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 2s 105ms/step - loss: 0.0340 - accuracy: 0.9889 - val_loss: 1.6733 - val_accuracy: 0.7234\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 2s 105ms/step - loss: 0.0266 - accuracy: 0.9871 - val_loss: 1.6142 - val_accuracy: 0.7234\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 2s 103ms/step - loss: 0.0327 - accuracy: 0.9926 - val_loss: 1.7490 - val_accuracy: 0.7092\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0403 - accuracy: 0.9889 - val_loss: 2.1187 - val_accuracy: 0.6596\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0464 - accuracy: 0.9834 - val_loss: 2.1375 - val_accuracy: 0.6809\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.1294 - accuracy: 0.9501 - val_loss: 1.1416 - val_accuracy: 0.7376\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 2s 104ms/step - loss: 0.0778 - accuracy: 0.9743 - val_loss: 1.3514 - val_accuracy: 0.7305\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 2s 104ms/step - loss: 0.0416 - accuracy: 0.9926 - val_loss: 1.6200 - val_accuracy: 0.7021\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 2s 105ms/step - loss: 0.0322 - accuracy: 0.9908 - val_loss: 1.7474 - val_accuracy: 0.7021\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 2s 122ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 1.4278 - val_accuracy: 0.7376\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 2s 135ms/step - loss: 0.0152 - accuracy: 0.9945 - val_loss: 1.8254 - val_accuracy: 0.6950\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.0146 - accuracy: 0.9963 - val_loss: 1.4443 - val_accuracy: 0.7660\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 2s 99ms/step - loss: 0.0066 - accuracy: 0.9982 - val_loss: 1.8578 - val_accuracy: 0.6879\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 2s 100ms/step - loss: 0.0219 - accuracy: 0.9908 - val_loss: 1.5971 - val_accuracy: 0.7447\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0598 - accuracy: 0.9797 - val_loss: 1.2171 - val_accuracy: 0.7518\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 2s 103ms/step - loss: 0.0381 - accuracy: 0.9834 - val_loss: 1.9298 - val_accuracy: 0.7092\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 2s 101ms/step - loss: 0.0412 - accuracy: 0.9815 - val_loss: 2.3469 - val_accuracy: 0.6809\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 2s 105ms/step - loss: 0.0509 - accuracy: 0.9852 - val_loss: 2.1403 - val_accuracy: 0.7092\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 2s 104ms/step - loss: 0.0451 - accuracy: 0.9852 - val_loss: 2.4121 - val_accuracy: 0.7021\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.0887 - accuracy: 0.9630 - val_loss: 2.0531 - val_accuracy: 0.7234\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 2s 101ms/step - loss: 0.1670 - accuracy: 0.9538 - val_loss: 1.5372 - val_accuracy: 0.7163\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 2s 102ms/step - loss: 0.1651 - accuracy: 0.9445 - val_loss: 1.5185 - val_accuracy: 0.7021\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 2s 103ms/step - loss: 0.0472 - accuracy: 0.9815 - val_loss: 2.0660 - val_accuracy: 0.7305\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 2s 103ms/step - loss: 0.0763 - accuracy: 0.9797 - val_loss: 2.6798 - val_accuracy: 0.6738\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.0645 - accuracy: 0.9797 - val_loss: 1.7657 - val_accuracy: 0.7163\n"
     ]
    }
   ],
   "source": [
    "if vgg_net:\n",
    "    from keras.layers import Input\n",
    "    from keras import applications\n",
    "    from keras.models import Model\n",
    "    input_tensor = Input(shape=(img_height, img_width, 3))\n",
    "    vgg_model = applications.VGG16(weights='imagenet',\n",
    "                                   include_top=False,\n",
    "                                   input_tensor=input_tensor)\n",
    "    # Creating dictionary that maps layer names to the layers\n",
    "    layer_dict = dict([(layer.name, layer) for layer in vgg_model.layers])\n",
    "\n",
    "    # Getting output tensor of the last VGG layer that we want to include\n",
    "    x = layer_dict['block2_pool'].output\n",
    "\n",
    "    # Stacking a new simple convolutional network on top of it\n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(num_classes, activation='softmax')(x)\n",
    "    custom_model = Model(vgg_model.input, x)\n",
    "    # Make sure that the pre-trained bottom layers are not trainable\n",
    "    for layer in custom_model.layers[:7]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Do not forget to compile it\n",
    "    custom_model.compile(loss='categorical_crossentropy',\n",
    "                         optimizer='Adam',\n",
    "                         metrics=['accuracy'])\n",
    "    custom_model.fit_generator(train_data_gen,\n",
    "                        steps_per_epoch=step_size_train,\n",
    "                        epochs=50,\n",
    "                        validation_data=validation_data_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "science",
   "language": "python",
   "name": "science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
